# Design Decisions & Open Questions

**Created:** 2025-10-18
**Last Updated:** 2025-10-18

This document tracks design decisions that need to be made before implementation. Each question will be answered and documented here as we progress.

---

## Status Legend
- üî¥ **Open** - Not yet decided
- üü° **In Discussion** - Currently being explored
- üü¢ **Decided** - Decision made and documented

---

## 1. Agent Communication Pattern üü¢

**Question:** How do agents share context with each other?

**Sub-questions:**
- Does each agent see the full conversation history or just relevant parts?
- How does the Evaluator get access to what Brainstormer produced?
- Do agents pass messages directly or through a shared state?

**Options:**
- A) Full conversation history shared with all agents
- B) Filtered/relevant context per agent role
- C) Explicit message passing between agents
- D) Shared state object that agents read/write to

**Decision:** **Hybrid: Shared State Object (D) + Filtered Context (B)**

**Implementation:**
```python
# Structured state for data
class SessionState(BaseModel):
    user_request: str
    plan: Optional[Plan] = None
    research_findings: List[Finding] = []
    generated_prompts: List[Prompt] = []
    evaluations: List[Evaluation] = []
    iteration: int = 0

# Each agent gets filtered conversation history
planner_context = [user_message]
researcher_context = [user_message, planner_response]
brainstormer_context = [user_message, research_summary]  # NOT full research process
evaluator_context = [user_message, generated_prompts]    # NOT brainstorming process
```

**Rationale:**
- **Structured data in shared state** - Easy to checkpoint, resume, and access specific outputs
- **Filtered conversation history** - Reduces token costs and prevents irrelevant context from distracting agents
- **Prevents bias** - Evaluator doesn't see Brainstormer's reasoning process, ensures objective scoring
- **Type-safe** - Pydantic models provide validation and clear schema
- **Efficient** - Only pass relevant context to each agent's LLM calls
- **Flexible** - Can adjust filtering strategy per agent as needed
- **Framework-friendly** - Works well with MS Agent Framework's state management

---

## 2. Scoring System Details üü¢

**Question:** How should solutions be scored?

**Sub-questions:**
- What's the scale? (1-10, 0-100, letter grades?)
- What criteria does the Evaluator use?
- Single score or multiple dimensions (creativity, feasibility, completeness)?
- Is scoring deterministic or LLM-based?

**Options:**
- A) Simple 1-10 single score (LLM-based)
- B) Multi-dimensional scoring (creativity: X, feasibility: Y, clarity: Z)
- C) Hybrid: LLM generates score + human can override
- D) Comparative ranking instead of absolute scores

**Decision:** **Multi-dimensional Scoring (B) with Human Override (C)**

**Implementation:**
```python
class Evaluation(BaseModel):
    prompt_id: str
    quality: float        # 0-10: General goodness
    clarity: float        # 0-10: Easy to understand/use
    specificity: float    # 0-10: Detailed enough
    overall: float        # Average of dimensions
    reasoning: str        # Explanation for each score
    human_override: Optional[float] = None  # User can override overall score

    @property
    def final_score(self) -> float:
        return self.human_override if self.human_override else self.overall

# Usage:
# Autonomous mode: Uses overall score, terminates at >= 9.0
# Interactive mode: User can override any score
```

**Scoring Dimensions (MVP):**
- **Quality** - General excellence, fit for purpose
- **Clarity** - Easy to understand and actionable
- **Specificity** - Sufficient detail and precision
- **Overall** - Average of the three dimensions

**Human Override:**
- In **interactive mode**: User can override any evaluation after seeing it
- In **autonomous mode**: Override disabled, uses LLM scores
- Overrides are saved for future learning (Phase 4 enhancement)

**Rationale:**
- **Multi-dimensional feedback** - Agents know exactly what to improve
- **Transparency** - User understands WHY a score was given
- **Iterative improvement** - Brainstormer can target weak dimensions
- **Human-in-the-loop** - User maintains control when desired
- **Flexible** - Can add use-case specific dimensions later
- **Data-rich** - Better for cost/benefit analysis and analytics
- **Autonomous-friendly** - Works without human when needed

---

## 3. CLI Commands & Interface üü¢

**Question:** What commands and interface will users interact with?

**Sub-questions:**
- What top-level commands? (e.g., `start`, `resume`, `list`, `export`)
- How are agent thoughts displayed during execution?
- Real-time streaming output or batch display after each step?
- How to show multiple agents working in parallel?

**Options:**
- A) Simple commands: `ideation start`, `ideation resume <session-id>`
- B) Rich subcommands: `ideation session new`, `ideation session resume`
- C) Interactive mode: Launch into interactive shell
- D) Combination: Support both CLI commands and interactive mode

**Decision:** **Interactive Shell Mode (C) - No CLI commands needed for MVP**

**Agent Pool Architecture:**
- **1 Planner** - Creates execution plan
- **1 Expert** - Clarifies requirements (conditional)
- **Multiple Researchers** (2-3) - Parallel research on different aspects
- **Multiple Brainstormers** (2-3) - Generate diverse solutions
- **1 Evaluator** - Scores all outputs

**Interface Design:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ideation Agent - Session abc123                   Step 3/10 [‚óè] Live‚îÇ
‚îÇ Mode: Autonomous | Cost: $0.23 | Time: 5m 12s                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ PROCESS HISTORY (Scrollable) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

[USER] 3:45 PM
Generate prompts for modern villa architecture

[Planner] 3:45 PM | Cost: $0.03
Created execution plan:
  1. Expert clarification (style, requirements)
  2. Parallel research (3 researchers: trends, materials, composition)
  3. Brainstorm prompts (3 variations: minimal, detailed, atmospheric)
  4. Evaluate and refine
‚úì Plan approved by user

[Expert] 3:46 PM | Cost: $0.05
Clarifying requirements...
Q: What architectural style? Modern minimalist or contemporary?
Q: Any specific materials to emphasize?
User response: "Minimalist with lots of glass and concrete"
‚úì Requirements clarified

[Researcher-1: Trends] 3:47 PM | Cost: $0.04
Researching modern villa trends...
Found 5 key trends:
  - Large glass facades for natural light
  - Open-plan living spaces
  - Integration with landscape
  - Sustainable materials
  - Minimalist clean lines
‚úì Research complete

[Researcher-2: Materials] 3:47 PM | Cost: $0.04  [Parallel]
Researching glass and concrete in architecture...
Best practices:
  - Floor-to-ceiling glass for transparency
  - Exposed concrete for brutalist aesthetic
  - Balance warmth with cold materials
‚úì Research complete

[Researcher-3: Composition] 3:48 PM | Cost: $0.05  [Parallel]
Researching architectural photography composition...
Key principles:
  - Golden hour lighting preferred
  - Eye-level or slightly elevated angles
  - Emphasis on geometry and lines
‚úì Research complete

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ NOW WORKING (Real-time) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ñ∂ [Brainstormer-1: Minimal] 3:49 PM | Cost: $0.06 (in progress)
Generating minimalist prompt variation...
"Modern minimalist villa, floor-to-ceiling glass facade, exposed
concrete walls, clean geometric lines, integrated with hillside
landscape, golden hour lighting, architectural photography..."

‚ñ∂ [Brainstormer-2: Detailed] 3:49 PM | Cost: $0.07 (in progress) [Parallel]
Generating detailed prompt variation...
"Ultra-modern luxury villa, 12-meter glass panels, raw concrete
structure, cantilevered design, infinity pool..."

‚ñ∂ [Brainstormer-3: Atmospheric] 3:49 PM | Cost: $0.06 (in progress) [Parallel]
Generating atmospheric prompt variation...
"Serene modern villa at dusk, warm interior lighting through floor..."

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Cost Breakdown:
  Planner: $0.03 | Expert: $0.05 | Researchers: $0.13 | Brainstormers: $0.19
  Total: $0.40 (Claude Sonnet 4.5)

Active Agents: Brainstormer-1, Brainstormer-2, Brainstormer-3
Waiting: Evaluator

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [Enter] Skip  [S] Stop & feedback  [P] Pause  [Q] Quit & save      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
> _
```

**Key Features:**
1. **Process History (Scrollable)** - Full conversation log showing everything that happened
   - Timestamps for each agent response
   - Cost per message
   - Completed work clearly marked with ‚úì
   - Shows parallel execution with [Parallel] tag
2. **Multiple Specialized Agents**
   - 2-3 Researchers with different focuses (trends, materials, composition)
   - 2-3 Brainstormers generating different variations (minimal, detailed, atmospheric)
   - Each agent labeled with their specialty
3. **Real-time "Now Working" Section** - Live streaming of active agents
   - Shows all parallel agents currently executing
   - In-progress output streams in real-time
4. **Comprehensive Cost Monitor** - Per-agent and total cost tracking
   - Individual agent costs visible in history
   - Aggregated cost breakdown by agent type
   - Running total with LLM provider
5. **Always Interruptible** - Press 'S' at any time to stop and give feedback
6. **Status Bar** - Quick glance: step, cost, time, active agents

**User Interactions:**
- **Launch:** Simply run `ideation` ‚Üí drops into interactive session
- **Start Session:** Prompted for topic, mode (interactive/autonomous), max iterations
- **During Execution:**
  - Press `S` to stop agents and give feedback
  - Press `Enter` to skip your turn (pass to agents)
  - Press `P` to pause (agents stop at current step)
  - Press `C` to continue if paused
  - Press `Q` to quit and save checkpoint
- **After Agent Response:**
  - View results in real-time
  - Override scores if in interactive mode
  - Provide feedback/direction

**Feedback Mechanism:**
```
[You pressed 'S' - Agents paused]

What would you like to do?
> 1. Give feedback/direction
> 2. Override last evaluation
> 3. Change mode (autonomous ‚Üî interactive)
> 4. Adjust max iterations
> 5. Continue

Your choice: 1

Enter feedback: "Focus more on sustainable materials and passive cooling"

[Feedback saved - Resuming with updated context]
```

**No CLI Commands for MVP:**
- Launch directly into interactive mode
- Session management (list/resume/export) can be added later
- Focus on the brainstorming experience first

**Rationale:**
- **User wanted interactive** - No need for CLI command complexity
- **Full process visibility** - Scrollable history shows entire journey from start
- **Multiple agents for diversity** - 2-3 researchers/brainstormers generate richer, more varied output
- **Parallel execution visible** - User sees multiple agents working simultaneously
- **Stop anytime** - 'S' key immediately pauses agents for feedback
- **Rich information display** - All relevant data visible: costs per message, timestamps, status, full conversation
- **Natural workflow** - Feels like watching a team collaborate, not commands
- **MVP-focused** - Simpler to implement than command infrastructure
- **Engaging** - Real-time updates keep user connected to agent work
- **Always in control** - Can interrupt, pause, or redirect at any moment
- **Transparency** - See what each specialized agent contributed

---

## 4. Configuration & Settings üî¥

**Question:** How should configuration be managed?

**Sub-questions:**
- Where are API keys stored? (.env file? config.yaml?)
- How to configure quality thresholds, max iterations, default modes?
- Where are agent personalities/system prompts defined?
- User-level vs. project-level configuration?

**Options:**
- A) `.env` for secrets, `config.yaml` for settings
- B) All in `.env` file
- C) Interactive first-time setup wizard
- D) Code-based config with optional file overrides

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 5. Error Handling & Recovery üî¥

**Question:** How to handle errors and failures?

**Sub-questions:**
- What happens if LLM API fails mid-session?
- What if an agent gets stuck in a loop?
- How to detect and recover from infinite loops?
- Retry strategies for API failures?

**Options:**
- A) Fail fast: Stop session, save checkpoint, let user resume
- B) Auto-retry with exponential backoff
- C) Fallback to simpler agent behavior
- D) User prompt: "API failed, retry or abort?"

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 6. Output Formats & Export üî¥

**Question:** How are results presented and exported?

**Sub-questions:**
- Final results format in CLI (table, list, panels?)
- Export formats (JSON, Markdown, PDF, HTML?)
- Include agent reasoning/thoughts in export?
- Session history format and storage?

**Options:**
- A) Rich CLI display + JSON export
- B) Rich CLI display + Markdown export
- C) Multiple export formats (JSON, MD, HTML)
- D) Customizable templates for export

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 7. Parallel Agent Execution üî¥

**Question:** How do multiple agents run concurrently?

**Sub-questions:**
- True parallel execution (async) or sequential with batching?
- How does synchronization work (wait for all, first-to-finish, streaming)?
- Does user see both agents working simultaneously?
- How to handle dependencies between parallel agents?

**Options:**
- A) True async parallel execution with Python asyncio
- B) Sequential execution but present as parallel in UI
- C) Parallel where possible, sequential where needed
- D) User chooses mode (fast parallel vs. observable sequential)

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 8. Plan Modification by User üî¥

**Question:** How can users modify the generated plan?

**Sub-questions:**
- Edit plan as text? Choose from alternative plans? Step-by-step approval?
- Can user add/remove steps from the plan?
- Can user reorder steps?
- Freeform text editing or structured modifications?

**Options:**
- A) Approve/Reject only (no modifications)
- B) Interactive step-by-step approval (approve/skip/modify each step)
- C) Text editor opens for freeform plan editing
- D) Structured prompts to add/remove/modify specific steps

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 9. LLM Provider üî¥

**Question:** Which LLM provider to use?

**Sub-questions:**
- Claude (Anthropic) vs GPT (OpenAI)?
- Support multiple providers from day 1?
- Model selection (GPT-4, Claude Sonnet 4.5, etc.)?
- Cost vs. quality tradeoffs?

**Options:**
- A) Claude only (Sonnet 4.5 for best reasoning)
- B) GPT only (GPT-4 for broad compatibility)
- C) Support both, user chooses in config
- D) Different models for different agents (cheap for simple, expensive for complex)

**Recommendation from previous discussion:** Start with Claude for best reasoning

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 10. Deployment Model üî¥

**Question:** How will the application be deployed?

**Sub-questions:**
- Local CLI only or client-server architecture?
- Single-user or multi-user?
- Where is state stored (local files, database, cloud)?

**Options:**
- A) Local CLI application, all state local (MVP approach)
- B) Local CLI + optional cloud sync
- C) Client-server architecture (CLI client, API server)
- D) Web app + API (future evolution)

**Recommendation from previous discussion:** MVP = Local CLI application

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 11. Configuration Management üî¥

**Question:** YAML files vs. Interactive setup vs. Code-based?

**Sub-questions:**
- How do users customize agent personas?
- Are prompt templates in code or config files?
- Version control for configurations?

**Options:**
- A) Code-based config (simple, version controlled)
- B) YAML files (flexible, non-technical users can edit)
- C) Interactive first-time setup wizard ‚Üí generates config
- D) Hybrid: Defaults in code, overrides in YAML

**Recommendation from previous discussion:** Start with code-based config, add YAML when needed

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 12. Prompt Templates üî¥

**Question:** Should prompt templates be hardcoded or user-customizable?

**Sub-questions:**
- How are agent system prompts defined?
- Can users customize agent personalities?
- Versioning of prompts?

**Options:**
- A) Hardcoded in code (simple, optimized)
- B) External files (YAML/JSON) (flexible)
- C) Database stored (dynamic updates)
- D) Hybrid: Defaults in code, user overrides in files

**Recommendation from previous discussion:** MVP = Hardcoded optimized prompts, Future = Allow customization

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 13. Cost Tracking & Monitoring üî¥

**Question:** How should LLM API costs be tracked and displayed?

**Sub-questions:**
- Track costs per message, per agent, per session, or all of the above?
- Real-time cost display or summary at the end?
- How to handle different LLM providers with different pricing?
- Store cost history for analysis?
- Set budget limits or warnings?

**Options:**
- A) Simple: Total session cost displayed at the end
- B) Detailed: Per-agent cost breakdown shown in real-time
- C) Comprehensive: Per-message tracking with running total + budget warnings
- D) Analytics: Full cost history stored in DB with reports and trends

**Display Options:**
- Show running cost in CLI footer/status bar
- Cost summary after each agent response
- Detailed cost report at session end
- Separate `ideation costs` command to view history

**Tracking Granularity:**
```
Session Level:
  ‚îî‚îÄ "Total cost: $0.45"

Agent Level:
  ‚îú‚îÄ Planner: $0.05
  ‚îú‚îÄ Researcher: $0.15
  ‚îú‚îÄ Brainstormer: $0.20
  ‚îî‚îÄ Evaluator: $0.05

Message Level:
  ‚îú‚îÄ Message 1 (Planner): $0.02 (input: 500 tokens, output: 300 tokens)
  ‚îú‚îÄ Message 2 (Researcher): $0.08 (input: 1200 tokens, output: 800 tokens)
  ‚îî‚îÄ ...

LLM Provider Breakdown:
  ‚îú‚îÄ Claude (Sonnet 4.5): $0.30
  ‚îî‚îÄ GPT-4: $0.15
```

**Implementation Considerations:**
- Need pricing tables for each provider/model
- Track input tokens vs output tokens (different pricing)
- Update pricing when providers change rates
- Exchange rate handling for non-USD pricing?

**Decision:** [To be decided]

**Rationale:** [To be filled]

---

## 14. Dynamic Agent Configuration & Optimization üî¥

**Question:** Should the system automatically optimize agent count, types, and execution strategy?

**Sub-questions:**
- How to determine optimal number of researchers and brainstormers?
- Should agent specializations be dynamic (AI-generated) or predefined?
- How to decide turn-taking strategy (sequential, parallel, hybrid)?
- Should configuration adapt based on use case or past performance?

**Options:**

### A) Hardcoded Configuration (Simple)
```python
# Fixed configuration per use case
IMAGE_PROMPTS_CONFIG = {
    "researchers": 3,  # trends, materials, composition
    "brainstormers": 3,  # minimal, detailed, atmospheric
    "parallel": True
}
```
**Pros:**
- ‚úÖ Simple, predictable
- ‚úÖ Fast to implement
- ‚úÖ Easy to debug

**Cons:**
- ‚ùå Not adaptive
- ‚ùå May be suboptimal for some cases

---

### B) Meta-Agent (AI Designs the Team)
```python
# Meta-planner decides configuration
[Meta-Planner] Analyzing task: "Generate architecture prompts"
  ‚Üí Need 3 researchers: trends, materials, lighting
  ‚Üí Need 2 brainstormers: minimalist, maximalist
  ‚Üí Strategy: Parallel research, sequential brainstorming
```
**Pros:**
- ‚úÖ Adaptive to task complexity
- ‚úÖ Can discover novel configurations
- ‚úÖ Uses LLM reasoning

**Cons:**
- ‚ö†Ô∏è Extra LLM call (cost + latency)
- ‚ö†Ô∏è Less predictable

---

### C) Rule-Based Optimizer
```python
# Rules based on task characteristics
if task_complexity == "high":
    researchers = 4
elif task_complexity == "medium":
    researchers = 3
else:
    researchers = 2

if need_diversity:
    brainstormers = 3
    parallel = True
```
**Pros:**
- ‚úÖ Deterministic
- ‚úÖ No extra LLM calls
- ‚úÖ Can encode domain knowledge

**Cons:**
- ‚ö†Ô∏è Requires manual rule tuning
- ‚ö†Ô∏è May miss edge cases

---

### D) Learning-Based (Phase 4 Enhancement)
```python
# Learn from past sessions what works best
# Store: (task_type, config, quality_score, cost, time)
# Use historical data to predict optimal config

Past data:
  architecture_prompts + 3 researchers + 2 brainstormers ‚Üí 8.5/10, $0.40, 5min
  architecture_prompts + 2 researchers + 3 brainstormers ‚Üí 7.0/10, $0.35, 4min

‚Üí Next time: Use 3 researchers + 2 brainstormers
```
**Pros:**
- ‚úÖ Improves over time
- ‚úÖ Data-driven optimization
- ‚úÖ Cost-effective

**Cons:**
- ‚ùå Requires historical data
- ‚ùå Complex implementation
- ‚ùå Phase 4 feature

---

### E) Hybrid: Meta-Agent + User Override
```python
# Meta-agent proposes configuration
[Meta-Planner] Proposed team:
  - 3 Researchers (trends, materials, composition)
  - 2 Brainstormers (minimal, detailed)
  - Parallel execution

Approve? (y/n/edit)
User: edit ‚Üí "Add one more brainstormer for atmospheric variation"

[Meta-Planner] Updated team:
  - 3 Researchers
  - 3 Brainstormers (minimal, detailed, atmospheric)
```
**Pros:**
- ‚úÖ Adaptive + user control
- ‚úÖ Best of both worlds
- ‚úÖ Educational for user

**Cons:**
- ‚ö†Ô∏è Requires user input
- ‚ö†Ô∏è Slower than autonomous

---

**Turn-Taking Strategies:**

1. **Full Parallel** - All researchers run simultaneously, all brainstormers run simultaneously
   - Fastest, highest diversity, highest cost

2. **Sequential** - One agent at a time
   - Slowest, most observable, cheapest

3. **Batched Parallel** - Researchers in parallel, then brainstormers in parallel
   - Balanced speed and cost

4. **Dynamic** - Meta-agent decides based on task
   - Most flexible, requires AI orchestration

---

**Decision:** [To be decided]

**Rationale:** [To be filled]

**Recommendation for MVP:**
- Start with **Option B (Meta-Agent)** for configuration
- Use **Batched Parallel** execution (research together, brainstorm together)
- Allow user to see and approve the proposed team
- Add learning-based optimization in Phase 4

**Example Flow:**
```
User: "Generate architecture prompts for modern villa"

[Meta-Planner] Analyzing task...
  Complexity: Medium
  Domain: Architecture + Image generation

  Proposed team:
    ‚úì 1 Expert (clarification)
    ‚úì 3 Researchers (parallel)
      - Trends researcher
      - Materials researcher
      - Composition researcher
    ‚úì 3 Brainstormers (parallel)
      - Minimalist variation
      - Detailed variation
      - Atmospheric variation
    ‚úì 1 Evaluator

  Execution: Batched parallel (researchers together, brainstormers together)
  Estimated cost: $0.40-0.60
  Estimated time: 5-7 minutes

Approve team? (y/n/edit): _
```

---

## Decision Process

When answering each question, we'll follow this process:

1. **Discuss options** - Explore pros/cons of each approach
2. **Consider MVP vs. Future** - What's needed now vs. later?
3. **Make decision** - Choose an option
4. **Document rationale** - Why this choice?
5. **Update status** - üî¥ ‚Üí üü° ‚Üí üü¢
6. **Update PROJECT_SPEC.md** - Add concrete details to main spec

---

## Notes

- Decisions should balance MVP speed with future flexibility
- Prefer simple solutions that can evolve over complex premature optimization
- Document tradeoffs and alternatives considered
- Revisit decisions as we learn more during implementation
